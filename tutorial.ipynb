{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eCV71QyL_IXH"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danperazzo/computational_creativity/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOD6IOpU-OW3"
      },
      "source": [
        "##### Tutorial do Grupo Separatistas\n",
        "\n",
        "Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7UXdLsn-8wD"
      },
      "source": [
        "# Copyright 2020 Daniel Perazzo, LeÃ£o Liu, Nara Andrade, Mayara Nunes\n",
        "# Mario Wassen, Aline Gouveia e Rebeca Silva \n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXhPIvAZlk6K"
      },
      "source": [
        "# Feature Visualization for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9IG6cz3lmOk"
      },
      "source": [
        "In the following tutorial we will use Feature Visualization techniques to create really cool abstract images. For this, we will use the [lucid visualition library](https://github.com/tensorflow/lucid) coupled with [Tensorflow at version 1.15](https://www.tensorflow.org/?hl=pt-br)\n",
        "\n",
        "![](https://storage.googleapis.com/tensorflow-lucid/notebooks/xy2rgb/cppn-header.jpg)\n",
        "This tutorial will teach you how to use Feature Visualizations for creating really cool abstract images. \n",
        "\n",
        "This tutorial is heavily based on the colab notebook provided in [this link](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb) and also the multiple lucid notebooks available on their repo (links at the end of the tutorial). Thanks for the authors for sharing the code with the Apache 2.0 License. If you want to know more about this topic, we recommend you to check out [this repo](https://github.com/tensorflow/lucid).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-pWWdoVTuNV"
      },
      "source": [
        "## Introduction: WTF are these things?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4Fbf6SUlK7"
      },
      "source": [
        "First off. What the hell are Feature Visualizations? \n",
        "To respond, let's imagine a classifier. We can imagine that the neurons fire up to detect the objects they are looking for. But, what are the neurons seeing? Why are they firing in such a way? \n",
        "\n",
        "Feature visualizations let us see what estimulates each neuron. The idea is, instead of optimizing the neural network's weights during to accomodate the input, in here we optimized the image to maximize the neuron's activation. In this tutorial we will see some ideas and create some cool visualizations.\n",
        "\n",
        "\n",
        "So in, the following section, we will see some neurons firing up and, finally, create some trippy images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOCfOPdCkmXW"
      },
      "source": [
        "## Install, Import, and load a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9g4eyrV8ZFz"
      },
      "source": [
        "In here we will just import and install the necessary modules, including Tensorflow and Lucid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cFl6KWwLoJ9"
      },
      "source": [
        "!pip uninstall numpy # You will have to type n\n",
        "!pip install numpy==1.16.1\n",
        "!pip install tf_slim\n",
        "!pip install -q lucid>=0.2.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTCveRwjx2x_"
      },
      "source": [
        "Now we will just import the necessary packages for our tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEzrNW5lBUxr"
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import string\n",
        "import PIL\n",
        "import base64\n",
        "from glob import glob\n",
        "import scipy.ndimage as nd\n",
        "\n",
        "import matplotlib.pylab as pl\n",
        "\n",
        "%tensorflow_version 1.x # Just in case... Lucid only works for Tensorflow == 1.x\n",
        "import tensorflow as tf\n",
        "import tf_slim as slim\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnwK7mYFFns"
      },
      "source": [
        "from lucid.modelzoo import vision_models\n",
        "from lucid.misc.io import show, save, load\n",
        "from lucid.optvis import objectives\n",
        "from lucid.optvis import transform\n",
        "from lucid.optvis import param\n",
        "from lucid.optvis import render\n",
        "from lucid.misc.tfutil import create_session\n",
        "from lucid.recipes import caricature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKDaWuQ_yBUD"
      },
      "source": [
        "## Getting Started: Neural visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t99zG8ni9Wpz"
      },
      "source": [
        "In this module, we will just be playing around and creating some images. To do this, we will optimize an image to fire up some neurons as much as possible. First, we will need to load a model just to see what we are talking about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRnDsiHa7ZK1"
      },
      "source": [
        "model = vision_models.InceptionV1()\n",
        "model.load_graphdef()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNZmPOA0Z_0"
      },
      "source": [
        "Now, with the model loaded, let's visualize what is happenning on channel 10 of layer \"mixed4a_pre_relu\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV8XmS482mTV"
      },
      "source": [
        "obj = objectives.channel(\"mixed4a_pre_relu\", 465)\n",
        "_ = render.render_vis(model, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEXnqq2Q3NmK"
      },
      "source": [
        "We can try different layers channels, in particular, we can see that if we put shallower layers, the images get simpler, as expected given how ConNets work. The output of this code is one of the things that you should deliver at the end of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEfSnre73pfO"
      },
      "source": [
        "# Just set the layer and channel like the previous example, changing the layer\n",
        "# for 'conv2d2' and the channel to 10\n",
        "###########YOUR CODE HERE################# \n",
        "obj = None\n",
        "###########YOUR CODE HERE################# \n",
        "_ = render.render_vis(model, obj)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eJmPkDgO6u-"
      },
      "source": [
        "Cool, we can also inspect how a specific neuron (ie: a specific x,y position on a specific layer on a specific channel) is estimulated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQBQyNR3PDM3"
      },
      "source": [
        "obj = objectives.neuron(\"conv2d2\", 10,x=10,y=10)\n",
        "_ = render.render_vis(model, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSWBQbU94XWM"
      },
      "source": [
        "## Some cool things you can do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k5dNd0s4fJj"
      },
      "source": [
        "On the introductory side, we can do some cool things with feature visualizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJfI-g7042iy"
      },
      "source": [
        "# We could optimize two neurons at the same time\n",
        "\n",
        "obj =objectives.channel(\"mixed4a_pre_relu\", 476) + objectives.channel(\"mixed4a_pre_relu\", 465)\n",
        "_ = render.render_vis(model, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikzo9dUk5_Do"
      },
      "source": [
        "# We could see two images: on the left we see the image that estimulates less the neuron. \n",
        "# On the right the image that estimulates the neuron more.\n",
        "\n",
        "param_f = lambda: param.image(128, batch=2)\n",
        "obj = objectives.channel(\"mixed4a_pre_relu\", 492, batch=1) - objectives.channel(\"mixed4a_pre_relu\", 492, batch=0)\n",
        "_ = render.render_vis(model, obj, param_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JWk8YkA7aem"
      },
      "source": [
        "#Finally, using techniques better explained on here: https://distill.pub/2017/feature-visualization/\n",
        "# We can better see this neuron firing on a diversity of settings\n",
        "\n",
        "param_f = lambda: param.image(128, batch=6)\n",
        "obj = obj = objectives.channel(\"mixed5a_pre_relu\", 9) - 1e2*objectives.diversity(\"mixed5a\")\n",
        "_ = render.render_vis(model, obj, param_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9UmtEi1exrb"
      },
      "source": [
        "## Getting started with Differentiable Image Parametrization: CPPNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvwbEXGoe3mM"
      },
      "source": [
        "Well we are over with the preliminaries, and let's see what we can do. Differentiable Image Parametrization is just a really cool way to parametize an image so that it is differentiable. In particular,in this section we will see how we can use Compositional Pattern Producing Networks(CPPNs) to create trippy images. \n",
        "\n",
        "CPPNs are just a network whose weights parametrize an image. Our idea is to optimize the weights so that our specified CNN channels gets as activated as possible. In doing this, we will create some trully great images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP5VDIXPe_SZ"
      },
      "source": [
        "# Let's load the CPPN fuction from lucid\n",
        "image_cppn = param.cppn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBmSPMnl5GW"
      },
      "source": [
        "In here we will again load a model. In a few moments you will play around with some of these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yhi_7AQfRy6"
      },
      "source": [
        "################HERE YOU WILL PLAY AROUND#################################\n",
        "#Test which models will go here, we recommend using the models: InceptionV1,\n",
        "model = vision_models.InceptionV1()\n",
        "#model = vision_models.VGG19_caffe()\n",
        "#model = vision_models.AlexNet()\n",
        "################HERE YOU WILL PLAY AROUND#################################\n",
        "model.load_graphdef()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRbDNQ6UfUJE"
      },
      "source": [
        "Just to give you a sense of how BIG this module is, it has exactly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhY50WOBfXB8",
        "outputId": "513b94df-b944-4365-8454-b946cf52ba7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Graph().as_default():\n",
        "  image_cppn(224)\n",
        "  variables = tf.get_collection('variables')\n",
        "  param_n = sum([v.shape.num_elements() for v in variables])\n",
        "  print('This amount of parameters:', param_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This amount of parameters: 8451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IycvP_I_fZqs"
      },
      "source": [
        "Now, let's go on and perform feature visualizations. The fun starts now! We will test our feature visualization framework with more examples. Here you will alter the neurons and different models, bellow is a list of images for you to test. Just substitute the string_obj for the required string.\n",
        "\n",
        "If you are testing on InceptionV1Net, try -> ''conv2d2', 'mixed4a_3x3_pre_relu' and 'head0_bottleneck'\n",
        "\n",
        "If you are testing on VGG19-, try -> 'conv3_3/conv3_3', 'conv5_3/conv5_3' and\n",
        "'fc7/fc7'\n",
        "\n",
        "If you are testing on AlexNet-, try -> 'Conv2D_2', 'Conv2D_5' and\n",
        "'Conv2D_7'\n",
        "\n",
        "To see the full list of other node names for these three models, or any other model you might want to check [this link](https://github.com/tensorflow/lucid/tree/master/lucid/modelzoo). \n",
        "As expected the deeper we get our neurons the more complex and interesting the images get.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTmMOheaffFL"
      },
      "source": [
        "###############MODIFY HERE ###############\n",
        "string_obj= None\n",
        "###############MODIFY HERE ###############\n",
        "\n",
        "def render_feature(\n",
        "    cppn_f = lambda: image_cppn(224),\n",
        "    optimizer = tf.train.AdamOptimizer(0.005),\n",
        "    objective = objectives.channel(string_obj, 20)):\n",
        "  vis = render.render_vis(model, objective, param_f=cppn_f, optimizer=optimizer, transforms=[], thresholds=[2**i for i in range(5,10)], verbose=False)\n",
        "  show(vis)\n",
        "\n",
        "render_feature()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3zF3tuzdUjQ"
      },
      "source": [
        "## Deep Caricature generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mHkVuUhC4rY"
      },
      "source": [
        "Well, now let's create dreamy versions of already existing objects. To do this, let's use Feature Inversions and use [these ideas](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/feature_inversion_caricatures.ipynb#scrollTo=RDCKv_xfviPQ). What we are going to do is similar to what is done with DeepDream. We will load the model and start the definitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XEmWQuiGsXS"
      },
      "source": [
        "# Import the InceptionV1 (GoogLeNet) model from the Lucid modelzoo\n",
        "\n",
        "model = vision_models.InceptionV1()\n",
        "model.load_graphdef()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F6iElwcGEw9"
      },
      "source": [
        "# Now we will just get the caricature function from the lucid package\n",
        "feature_inversion = caricature.feature_inversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBeeqIq7GWTN"
      },
      "source": [
        "Now, let's just choose some layers and see what their neurons are seeing when they look at the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVk3GQ4HG8ft"
      },
      "source": [
        "img = load(\"https://storage.googleapis.com/lucid-static/building-blocks/examples/dog_cat.png\")\n",
        "\n",
        "layers = ['conv2d%d' % i for i in range(0, 3)] + ['mixed3a', 'mixed3b', \n",
        "                                                  'mixed4a', 'mixed4b', 'mixed4c', 'mixed4d', 'mixed4e',\n",
        "                                                  'mixed5a', 'mixed5b']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT8t3y-eHBSt"
      },
      "source": [
        "for layer in layers:\n",
        "  print(layer)\n",
        "  caricature.feature_inversion(img, model,layer=layer)\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiXPjWU7HFv4"
      },
      "source": [
        "Just for the fun, you can now just choose some image, preferentially your photo, and upload to see how some neuron sees you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzON-so_HTbi"
      },
      "source": [
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  img = load(fn)\n",
        "  show(imgToModelSize(img))\n",
        "  feature_inversion(img, layer='mixed4d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCV71QyL_IXH"
      },
      "source": [
        "##Goodbye\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imnDsH-6AGZH"
      },
      "source": [
        "Sadly this really small tutorial just ended. However, you can check out these awesome links:\n",
        "\n",
        "###Repositories\n",
        "* [Lucid](https://github.com/tensorflow/lucid/)\n",
        "\n",
        "###Recomended Reading\n",
        "\n",
        "* [Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
        "* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)\n",
        "* [Using Artiï¬cial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)\n",
        "* [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)\n",
        "* [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/)\n",
        "* [Activation Atlas](https://distill.pub/2019/activation-atlas/)\n",
        "\n",
        "### Related Talks\n",
        "* [Lessons from a year of Distill ML Research](https://www.youtube.com/watch?v=jlZsgUZaIyY) (Shan Carter, OpenVisConf)\n",
        "* [Machine Learning for Visualization](https://www.youtube.com/watch?v=6n-kCYn0zxU) (Ian Johnson, OpenVisConf)\n",
        "\n",
        "Since we have to grade you, we ask you to submit the images generated varying the parameters of the CPPN. And submit also the image that you used to generate the DeepDreamish images."
      ]
    }
  ]
}